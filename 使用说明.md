# AIDocGenius 使用说明

## 目录

- [快速开始](#快速开始)
- [核心功能详解](#核心功能详解)
- [API 参考](#api-参考)
- [Web 界面使用](#web-界面使用)
- [CLI 使用](#cli-使用)
- [高级用法](#高级用法)
- [最佳实践](#最佳实践)

## 快速开始

### 安装

一行命令开箱即用：

```bash
pip install -r requirements.txt && python app.py
```

访问 http://localhost:8000

```bash
# 克隆项目
git clone https://github.com/jiangmuran/AIDocGenius.git
cd AIDocGenius

# 安装依赖
pip install -r requirements.txt
```

**可选依赖说明：**

- `transformers` + `torch`: 小模型摘要（首次使用自动下载模型）
- `PyPDF2`: PDF 文本提取
- `pyyaml`: YAML 读写
- `markdown`: 更高质量的 Markdown → HTML

或者在 Windows 上直接运行 `安装依赖.bat`

### 第一个程序

```python
from AIDocGenius import DocProcessor

# 创建处理器
processor = DocProcessor()

# 生成摘要
summary = processor.generate_summary("your_document.txt")
print(summary)
```

## 核心功能详解

### 1. 文档摘要生成

**功能说明：** 自动提取文档核心内容，生成简洁准确的摘要。

**基本用法：**

```python
from AIDocGenius import DocProcessor

processor = DocProcessor()

# 基本摘要生成
summary = processor.generate_summary("document.txt")

# 控制摘要长度
short_summary = processor.generate_summary(
    "document.txt",
    max_length=100,  # 最大长度
    min_length=50    # 最小长度
)
```

**小模型摘要（可选）：**

```python
processor = DocProcessor(config={
    "summarizer": {
        "use_small_model": True,
        "model_name": "google/flan-t5-small"
    }
})

summary = processor.generate_summary("document.txt", max_length=200)
```

**参数说明：**

- `document_path`: 文档路径（支持 txt, md, docx, pdf 等格式）
- `max_length`: 摘要最大长度（字符数）
- `min_length`: 摘要最小长度（字符数）

**示例：**

```python
# 为长篇文章生成不同长度的摘要
article = "long_article.txt"

# 超短摘要（标题）
title = processor.generate_summary(article, max_length=50)

# 短摘要（推荐）
brief = processor.generate_summary(article, max_length=150)

# 详细摘要
detailed = processor.generate_summary(article, max_length=500)
```

### 2. 多语言翻译

**功能说明：** 支持 40+ 种语言的互译，基于 Google Translate。

**基本用法：**

```python
# 中译英
translation = processor.translate(
    "chinese_doc.txt",
    target_language="en",
    source_language="zh"
)

# 英译中
translation = processor.translate(
    "english_doc.txt",
    target_language="zh",
    source_language="en"
)

# 自动检测源语言（可选）
translation = processor.translate(
    "document.txt",
    target_language="en"
)
```

自动检测依赖 Google Translate；若不可用，请显式传入 `source_language`。

**支持的语言代码：**

| 语言 | 代码 | 语言 | 代码 |
|------|------|------|------|
| 中文 | zh | 英语 | en |
| 日语 | ja | 韩语 | ko |
| 法语 | fr | 德语 | de |
| 西班牙语 | es | 俄语 | ru |
| 阿拉伯语 | ar | 意大利语 | it |
| 葡萄牙语 | pt | 越南语 | vi |
| 泰语 | th | 印尼语 | id |
| 印地语 | hi | 等等... | ... |

**实用示例：**

```python
# 批量翻译多个文件
from pathlib import Path

input_dir = Path("docs_zh")
output_dir = Path("docs_en")
output_dir.mkdir(exist_ok=True)

for doc in input_dir.glob("*.txt"):
    result = processor.translate(doc, "en", "zh")
    output_file = output_dir / doc.name
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(result)
```

### 3. 文档分析

**功能说明：** 全面分析文档的可读性、结构、关键词和统计信息。

**基本用法：**

```python
# 完整分析
analysis = processor.analyze("document.txt")

# 指定分析维度
analysis = processor.analyze(
    "document.txt",
    criteria=["readability", "keywords"]
)
```

**分析维度：**

1. **可读性分析 (readability)**
   - 可读性评分（0-100）
   - 平均句子长度
   - 平均词长
   - 改进建议

2. **结构分析 (structure)**
   - 段落数量
   - 句子数量
   - 标题数量
   - 结构评分

3. **关键词提取 (keywords)**
   - 提取前 N 个关键词
   - 词频统计

4. **统计信息 (statistics)**
   - 字符总数
   - 单词总数
   - 句子总数
   - 段落总数

**示例：**

```python
import json

# 分析文档
analysis = processor.analyze("report.txt")

# 查看可读性
readability = analysis['readability']
print(f"可读性评分: {readability['score']}/100")
print(f"建议: {readability['suggestion']}")

# 查看统计信息
stats = analysis['statistics']
print(f"总字数: {stats['word_count']}")
print(f"总句数: {stats['sentence_count']}")

# 查看关键词
print("关键词:")
for kw in analysis['keywords'][:5]:
    print(f"  - {kw['word']}: {kw['frequency']} 次")

# 保存分析报告
with open("analysis.json", "w", encoding="utf-8") as f:
    json.dump(analysis, f, ensure_ascii=False, indent=2)
```

### 4. 格式转换

**功能说明：** 在多种文档格式之间转换。

**支持的转换：**

- TXT ↔ Markdown
- Markdown → HTML
- TXT/Markdown → Word (DOCX)
- 任意格式 → JSON/YAML

**基本用法：**

```python
# Markdown 转 HTML
processor.convert("README.md", "README.html")

# 文本转 Word
processor.convert("article.txt", "article.docx")

# Markdown 转 JSON
processor.convert("data.md", "data.json")

# 带格式选项
processor.convert(
    "README.md",
    "README.html",
    format_options={"from_markdown": True}
)
```

**批量转换示例：**

```python
from pathlib import Path

# 批量转换 Markdown 到 HTML
for md_file in Path("docs").glob("*.md"):
    html_file = md_file.with_suffix(".html")
    processor.convert(md_file, html_file)
    print(f"已转换: {md_file} → {html_file}")
```

### 5. 批量处理

**功能说明：** 批量处理多个文档，支持多个操作。

**基本用法：**

```python
# 批量处理多个文档
results = processor.batch_process(
    input_dir="input_docs/",
    output_dir="output_results/",
    operations=["summarize", "analyze", "translate"],
    target_language="en",  # 翻译参数
    max_length=200         # 摘要参数
)

# 查看结果
for file_path, result in results.items():
    print(f"\n文件: {file_path}")
    print(f"摘要: {result.get('summary', 'N/A')[:100]}...")
    print(f"翻译: {result.get('translation', 'N/A')[:100]}...")
```

批量处理会在 `output_dir` 写入结果文件：

- `*.summary.txt`
- `*.translated.<lang>.txt`
- `*.analysis.json`
- `*.<output_format>`（convert 操作）
- `batch_report.json`、`batch_report.md`、`batch_report.csv`

使用 `report_prefix` 可以避免覆盖已有报告文件。

**支持的操作：**

- `summarize`: 生成摘要
- `translate`: 翻译文档
- `analyze`: 分析文档

## API 参考

### DocProcessor 类

主要的文档处理器类。

#### 初始化

```python
processor = DocProcessor(config=None)
```

#### 方法

##### generate_summary()

生成文档摘要。

```python
generate_summary(
    document_path: str,
    max_length: int = None,
    min_length: int = None
) -> str
```

##### translate()

翻译文档。

```python
translate(
    document_path: str,
    target_language: str,
    source_language: str = None
) -> str
```

##### analyze()

分析文档。

```python
analyze(
    document_path: str,
    criteria: List[str] = None
) -> dict
```

##### convert()

转换文档格式。

```python
convert(
    input_path: str,
    output_path: str,
    format_options: dict = None
) -> None
```

##### batch_process()

批量处理文档。

```python
batch_process(
    input_dir: str,
    output_dir: str,
    operations: List[str],
    report: bool = False,
    report_formats: List[str] = None,
    **kwargs
) -> dict
```

### REST API 端点

```
POST /summarize
POST /translate
POST /analyze
POST /convert
POST /compare
POST /merge
POST /batch
GET  /health
```

`/batch` 支持 `zip_output=true` 直接下载 zip 包，`report=true` 生成报告文件。

JSON 端点统一返回 `status/data/error/request_id` 结构。

`/batch` 配合 `zip_output=false` 会直接返回报告结构。

## Web 界面使用

### 启动服务

**Windows：**
```
双击 启动服务.bat
```

**其他系统：**
```bash
python app.py
```

### 访问界面

在浏览器打开：http://localhost:8000

### 功能说明

1. **上传文档**
   - 点击上传区域或拖放文件
   - 支持多种格式

2. **选择操作**
   - 生成摘要
   - 翻译文档（选择目标语言）
   - 文档分析
   - 格式转换（选择输出格式）

3. **查看结果**
   - 结果显示在页面下方
   - 格式转换会提供文件下载链接

## CLI 使用

```bash
# 摘要
python -m AIDocGenius.cli summary "document.txt" --max-length 200

# 分析
python -m AIDocGenius.cli analyze "document.txt" --output analysis.json

# 转换
python -m AIDocGenius.cli convert "README.md" "README.html"

# 比较
python -m AIDocGenius.cli compare "doc1.txt" "doc2.txt" --output diff.json

# 合并
python -m AIDocGenius.cli merge "a.txt" "b.txt" --output merged.txt --smart

# 批处理
python -m AIDocGenius.cli batch "input" "output" --operations summarize,analyze --max-length 200

# 预热小模型
python -m AIDocGenius.cli model warmup --model-name "google/flan-t5-small"

# 仅输出报告
python -m AIDocGenius.cli batch "input" "output" --operations summarize,analyze --report --report-only
```

支持通过 `--config` 传入 JSON/YAML 配置文件：

```bash
python -m AIDocGenius.cli summary "document.txt" --config config.json
```

## 高级用法

### 自定义配置

```python
config = {
    "summarizer": {
        "use_simple": True,
        "use_small_model": False,
        "model_name": "google/flan-t5-small"
    }
}

processor = DocProcessor(config=config)
```

也可以保存为 `config.json` / `config.yaml` 并在 CLI 使用：

```json
{
  "summarizer": {
    "use_small_model": true,
    "model_name": "google/flan-t5-small"
  }
}
```

环境变量：

- `MODEL_CACHE_DIR`: HuggingFace 模型缓存目录
- `MAX_UPLOAD_SIZE`: 上传大小上限（字节，默认 20971520）
- `CORS_ORIGINS`: 允许的来源列表（逗号分隔，默认 `*`）

### 处理多种文档类型

```python
from pathlib import Path

def process_all_documents(directory):
    """处理目录下的所有文档"""
    processor = DocProcessor()
    results = {}
    
    for file in Path(directory).rglob("*"):
        if file.suffix in ['.txt', '.md', '.docx', '.pdf']:
            try:
                # 生成摘要
                summary = processor.generate_summary(file, max_length=200)
                # 分析文档
                analysis = processor.analyze(file)
                
                results[str(file)] = {
                    'summary': summary,
                    'word_count': analysis['statistics']['word_count'],
                    'readability': analysis['readability']['score']
                }
            except Exception as e:
                results[str(file)] = {'error': str(e)}
    
    return results
```

### 创建文档报告

```python
def create_document_report(document_path, output_path):
    """创建完整的文档分析报告"""
    processor = DocProcessor()
    
    # 获取分析结果
    analysis = processor.analyze(document_path)
    summary = processor.generate_summary(document_path, max_length=300)
    
    # 生成报告
    report = f"""
# 文档分析报告

## 文档信息
- 文件名: {Path(document_path).name}
- 字数: {analysis['statistics']['word_count']}
- 句数: {analysis['statistics']['sentence_count']}

## 摘要
{summary}

## 可读性分析
- 评分: {analysis['readability']['score']}/100
- 建议: {analysis['readability']['suggestion']}

## 关键词
"""
    for kw in analysis['keywords'][:10]:
        report += f"- {kw['word']} ({kw['frequency']} 次)\n"
    
    # 保存报告
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(report)
```

## 最佳实践

### 1. 处理大文件

```python
# 对于大文件，建议分段处理
def process_large_file(file_path, chunk_size=5000):
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    chunks = [content[i:i+chunk_size] 
              for i in range(0, len(content), chunk_size)]
    
    summaries = []
    for chunk in chunks:
        # 保存临时文件
        with open("temp_chunk.txt", "w", encoding="utf-8") as f:
            f.write(chunk)
        
        summary = processor.generate_summary("temp_chunk.txt", max_length=200)
        summaries.append(summary)
    
    return " ".join(summaries)
```

### 2. 错误处理

```python
def safe_process(document_path):
    """带错误处理的文档处理"""
    try:
        processor = DocProcessor()
        result = {
            'status': 'success',
            'summary': processor.generate_summary(document_path),
            'analysis': processor.analyze(document_path)
        }
    except FileNotFoundError:
        result = {'status': 'error', 'message': '文件不存在'}
    except Exception as e:
        result = {'status': 'error', 'message': str(e)}
    
    return result
```

### 3. 性能优化

```python
# 复用 processor 实例
processor = DocProcessor()

# 批量处理时使用批量方法
results = processor.batch_process(
    input_dir="docs/",
    output_dir="results/",
    operations=["summarize", "analyze"]
)

# 而不是逐个处理
# for doc in documents:  # 不推荐
#     result = processor.generate_summary(doc)
```

## 故障排除

### 常见问题

**Q: 翻译功能报错？**
A: 确保网络连接正常，Google Translate API 可访问。

**Q: PDF 提取乱码？**
A: 某些 PDF 格式可能不完全支持，建议先转换为文本格式。

**Q: 内存不足？**
A: 处理大文件时，使用分段处理方法。

**Q: 摘要质量不佳？**
A: 调整 max_length 参数，或确保原文档格式清晰。

## 更多资源

- [快速上手指南](QUICKSTART.md)
- [示例代码](examples/)
- [问题反馈](https://github.com/jiangmuran/AIDocGenius/issues)
- [联系我们](mailto:jmr@jiangmuran.com)
