# AIDocGenius ä½¿ç”¨è¯´æ˜ ğŸ“–

## ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ ¸å¿ƒåŠŸèƒ½è¯¦è§£](#æ ¸å¿ƒåŠŸèƒ½è¯¦è§£)
- [API å‚è€ƒ](#api-å‚è€ƒ)
- [Web ç•Œé¢ä½¿ç”¨](#web-ç•Œé¢ä½¿ç”¨)
- [é«˜çº§ç”¨æ³•](#é«˜çº§ç”¨æ³•)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/jiangmuran/AIDocGenius.git
cd AIDocGenius

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

**å¯é€‰ä¾èµ–è¯´æ˜ï¼š**

- `transformers` + `torch`: å°æ¨¡å‹æ‘˜è¦ï¼ˆé¦–æ¬¡ä½¿ç”¨è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼‰
- `PyPDF2`: PDF æ–‡æœ¬æå–
- `pyyaml`: YAML è¯»å†™
- `markdown`: æ›´é«˜è´¨é‡çš„ Markdown â†’ HTML

æˆ–è€…åœ¨ Windows ä¸Šç›´æ¥è¿è¡Œ `å®‰è£…ä¾èµ–.bat`

### ç¬¬ä¸€ä¸ªç¨‹åº

```python
from AIDocGenius import DocProcessor

# åˆ›å»ºå¤„ç†å™¨
processor = DocProcessor()

# ç”Ÿæˆæ‘˜è¦
summary = processor.generate_summary("your_document.txt")
print(summary)
```

## æ ¸å¿ƒåŠŸèƒ½è¯¦è§£

### 1. æ–‡æ¡£æ‘˜è¦ç”Ÿæˆ

**åŠŸèƒ½è¯´æ˜ï¼š** è‡ªåŠ¨æå–æ–‡æ¡£æ ¸å¿ƒå†…å®¹ï¼Œç”Ÿæˆç®€æ´å‡†ç¡®çš„æ‘˜è¦ã€‚

**åŸºæœ¬ç”¨æ³•ï¼š**

```python
from AIDocGenius import DocProcessor

processor = DocProcessor()

# åŸºæœ¬æ‘˜è¦ç”Ÿæˆ
summary = processor.generate_summary("document.txt")

# æ§åˆ¶æ‘˜è¦é•¿åº¦
short_summary = processor.generate_summary(
    "document.txt",
    max_length=100,  # æœ€å¤§é•¿åº¦
    min_length=50    # æœ€å°é•¿åº¦
)
```

**å°æ¨¡å‹æ‘˜è¦ï¼ˆå¯é€‰ï¼‰ï¼š**

```python
processor = DocProcessor(config={
    "summarizer": {
        "use_small_model": True,
        "model_name": "google/flan-t5-small"
    }
})

summary = processor.generate_summary("document.txt", max_length=200)
```

**å‚æ•°è¯´æ˜ï¼š**

- `document_path`: æ–‡æ¡£è·¯å¾„ï¼ˆæ”¯æŒ txt, md, docx, pdf ç­‰æ ¼å¼ï¼‰
- `max_length`: æ‘˜è¦æœ€å¤§é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
- `min_length`: æ‘˜è¦æœ€å°é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰

**ç¤ºä¾‹ï¼š**

```python
# ä¸ºé•¿ç¯‡æ–‡ç« ç”Ÿæˆä¸åŒé•¿åº¦çš„æ‘˜è¦
article = "long_article.txt"

# è¶…çŸ­æ‘˜è¦ï¼ˆæ ‡é¢˜ï¼‰
title = processor.generate_summary(article, max_length=50)

# çŸ­æ‘˜è¦ï¼ˆæ¨èï¼‰
brief = processor.generate_summary(article, max_length=150)

# è¯¦ç»†æ‘˜è¦
detailed = processor.generate_summary(article, max_length=500)
```

### 2. å¤šè¯­è¨€ç¿»è¯‘

**åŠŸèƒ½è¯´æ˜ï¼š** æ”¯æŒ 40+ ç§è¯­è¨€çš„äº’è¯‘ï¼ŒåŸºäº Google Translateã€‚

**åŸºæœ¬ç”¨æ³•ï¼š**

```python
# ä¸­è¯‘è‹±
translation = processor.translate(
    "chinese_doc.txt",
    target_language="en",
    source_language="zh"
)

# è‹±è¯‘ä¸­
translation = processor.translate(
    "english_doc.txt",
    target_language="zh",
    source_language="en"
)

# è‡ªåŠ¨æ£€æµ‹æºè¯­è¨€ï¼ˆå¯é€‰ï¼‰
translation = processor.translate(
    "document.txt",
    target_language="en"
)
```

è‡ªåŠ¨æ£€æµ‹ä¾èµ– Google Translateï¼›è‹¥ä¸å¯ç”¨ï¼Œè¯·æ˜¾å¼ä¼ å…¥ `source_language`ã€‚

**æ”¯æŒçš„è¯­è¨€ä»£ç ï¼š**

| è¯­è¨€ | ä»£ç  | è¯­è¨€ | ä»£ç  |
|------|------|------|------|
| ä¸­æ–‡ | zh | è‹±è¯­ | en |
| æ—¥è¯­ | ja | éŸ©è¯­ | ko |
| æ³•è¯­ | fr | å¾·è¯­ | de |
| è¥¿ç­ç‰™è¯­ | es | ä¿„è¯­ | ru |
| é˜¿æ‹‰ä¼¯è¯­ | ar | æ„å¤§åˆ©è¯­ | it |
| è‘¡è„ç‰™è¯­ | pt | è¶Šå—è¯­ | vi |
| æ³°è¯­ | th | å°å°¼è¯­ | id |
| å°åœ°è¯­ | hi | ç­‰ç­‰... | ... |

**å®ç”¨ç¤ºä¾‹ï¼š**

```python
# æ‰¹é‡ç¿»è¯‘å¤šä¸ªæ–‡ä»¶
from pathlib import Path

input_dir = Path("docs_zh")
output_dir = Path("docs_en")
output_dir.mkdir(exist_ok=True)

for doc in input_dir.glob("*.txt"):
    result = processor.translate(doc, "en", "zh")
    output_file = output_dir / doc.name
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(result)
```

### 3. æ–‡æ¡£åˆ†æ

**åŠŸèƒ½è¯´æ˜ï¼š** å…¨é¢åˆ†ææ–‡æ¡£çš„å¯è¯»æ€§ã€ç»“æ„ã€å…³é”®è¯å’Œç»Ÿè®¡ä¿¡æ¯ã€‚

**åŸºæœ¬ç”¨æ³•ï¼š**

```python
# å®Œæ•´åˆ†æ
analysis = processor.analyze("document.txt")

# æŒ‡å®šåˆ†æç»´åº¦
analysis = processor.analyze(
    "document.txt",
    criteria=["readability", "keywords"]
)
```

**åˆ†æç»´åº¦ï¼š**

1. **å¯è¯»æ€§åˆ†æ (readability)**
   - å¯è¯»æ€§è¯„åˆ†ï¼ˆ0-100ï¼‰
   - å¹³å‡å¥å­é•¿åº¦
   - å¹³å‡è¯é•¿
   - æ”¹è¿›å»ºè®®

2. **ç»“æ„åˆ†æ (structure)**
   - æ®µè½æ•°é‡
   - å¥å­æ•°é‡
   - æ ‡é¢˜æ•°é‡
   - ç»“æ„è¯„åˆ†

3. **å…³é”®è¯æå– (keywords)**
   - æå–å‰ N ä¸ªå…³é”®è¯
   - è¯é¢‘ç»Ÿè®¡

4. **ç»Ÿè®¡ä¿¡æ¯ (statistics)**
   - å­—ç¬¦æ€»æ•°
   - å•è¯æ€»æ•°
   - å¥å­æ€»æ•°
   - æ®µè½æ€»æ•°

**ç¤ºä¾‹ï¼š**

```python
import json

# åˆ†ææ–‡æ¡£
analysis = processor.analyze("report.txt")

# æŸ¥çœ‹å¯è¯»æ€§
readability = analysis['readability']
print(f"å¯è¯»æ€§è¯„åˆ†: {readability['score']}/100")
print(f"å»ºè®®: {readability['suggestion']}")

# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
stats = analysis['statistics']
print(f"æ€»å­—æ•°: {stats['word_count']}")
print(f"æ€»å¥æ•°: {stats['sentence_count']}")

# æŸ¥çœ‹å…³é”®è¯
print("å…³é”®è¯:")
for kw in analysis['keywords'][:5]:
    print(f"  - {kw['word']}: {kw['frequency']} æ¬¡")

# ä¿å­˜åˆ†ææŠ¥å‘Š
with open("analysis.json", "w", encoding="utf-8") as f:
    json.dump(analysis, f, ensure_ascii=False, indent=2)
```

### 4. æ ¼å¼è½¬æ¢

**åŠŸèƒ½è¯´æ˜ï¼š** åœ¨å¤šç§æ–‡æ¡£æ ¼å¼ä¹‹é—´è½¬æ¢ã€‚

**æ”¯æŒçš„è½¬æ¢ï¼š**

- TXT â†” Markdown
- Markdown â†’ HTML
- TXT/Markdown â†’ Word (DOCX)
- ä»»æ„æ ¼å¼ â†’ JSON/YAML

**åŸºæœ¬ç”¨æ³•ï¼š**

```python
# Markdown è½¬ HTML
processor.convert("README.md", "README.html")

# æ–‡æœ¬è½¬ Word
processor.convert("article.txt", "article.docx")

# Markdown è½¬ JSON
processor.convert("data.md", "data.json")

# å¸¦æ ¼å¼é€‰é¡¹
processor.convert(
    "README.md",
    "README.html",
    format_options={"from_markdown": True}
)
```

**æ‰¹é‡è½¬æ¢ç¤ºä¾‹ï¼š**

```python
from pathlib import Path

# æ‰¹é‡è½¬æ¢ Markdown åˆ° HTML
for md_file in Path("docs").glob("*.md"):
    html_file = md_file.with_suffix(".html")
    processor.convert(md_file, html_file)
    print(f"å·²è½¬æ¢: {md_file} â†’ {html_file}")
```

### 5. æ‰¹é‡å¤„ç†

**åŠŸèƒ½è¯´æ˜ï¼š** æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£ï¼Œæ”¯æŒå¤šä¸ªæ“ä½œã€‚

**åŸºæœ¬ç”¨æ³•ï¼š**

```python
# æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£
results = processor.batch_process(
    input_dir="input_docs/",
    output_dir="output_results/",
    operations=["summarize", "analyze", "translate"],
    target_language="en",  # ç¿»è¯‘å‚æ•°
    max_length=200         # æ‘˜è¦å‚æ•°
)

# æŸ¥çœ‹ç»“æœ
for file_path, result in results.items():
    print(f"\næ–‡ä»¶: {file_path}")
    print(f"æ‘˜è¦: {result.get('summary', 'N/A')[:100]}...")
    print(f"ç¿»è¯‘: {result.get('translation', 'N/A')[:100]}...")
```

æ‰¹é‡å¤„ç†ä¼šåœ¨ `output_dir` å†™å…¥ç»“æœæ–‡ä»¶ï¼š

- `*.summary.txt`
- `*.translated.<lang>.txt`
- `*.analysis.json`
- `*.<output_format>`ï¼ˆconvert æ“ä½œï¼‰

**æ”¯æŒçš„æ“ä½œï¼š**

- `summarize`: ç”Ÿæˆæ‘˜è¦
- `translate`: ç¿»è¯‘æ–‡æ¡£
- `analyze`: åˆ†ææ–‡æ¡£

## API å‚è€ƒ

### DocProcessor ç±»

ä¸»è¦çš„æ–‡æ¡£å¤„ç†å™¨ç±»ã€‚

#### åˆå§‹åŒ–

```python
processor = DocProcessor(config=None)
```

#### æ–¹æ³•

##### generate_summary()

ç”Ÿæˆæ–‡æ¡£æ‘˜è¦ã€‚

```python
generate_summary(
    document_path: str,
    max_length: int = None,
    min_length: int = None
) -> str
```

##### translate()

ç¿»è¯‘æ–‡æ¡£ã€‚

```python
translate(
    document_path: str,
    target_language: str,
    source_language: str = None
) -> str
```

##### analyze()

åˆ†ææ–‡æ¡£ã€‚

```python
analyze(
    document_path: str,
    criteria: List[str] = None
) -> dict
```

##### convert()

è½¬æ¢æ–‡æ¡£æ ¼å¼ã€‚

```python
convert(
    input_path: str,
    output_path: str,
    format_options: dict = None
) -> None
```

##### batch_process()

æ‰¹é‡å¤„ç†æ–‡æ¡£ã€‚

```python
batch_process(
    input_dir: str,
    output_dir: str,
    operations: List[str],
    **kwargs
) -> dict
```

## Web ç•Œé¢ä½¿ç”¨

### å¯åŠ¨æœåŠ¡

**Windowsï¼š**
```
åŒå‡» å¯åŠ¨æœåŠ¡.bat
```

**å…¶ä»–ç³»ç»Ÿï¼š**
```bash
python app.py
```

### è®¿é—®ç•Œé¢

åœ¨æµè§ˆå™¨æ‰“å¼€ï¼šhttp://localhost:8000

### åŠŸèƒ½è¯´æ˜

1. **ä¸Šä¼ æ–‡æ¡£**
   - ç‚¹å‡»ä¸Šä¼ åŒºåŸŸæˆ–æ‹–æ”¾æ–‡ä»¶
   - æ”¯æŒå¤šç§æ ¼å¼

2. **é€‰æ‹©æ“ä½œ**
   - ç”Ÿæˆæ‘˜è¦
   - ç¿»è¯‘æ–‡æ¡£ï¼ˆé€‰æ‹©ç›®æ ‡è¯­è¨€ï¼‰
   - æ–‡æ¡£åˆ†æ
   - æ ¼å¼è½¬æ¢ï¼ˆé€‰æ‹©è¾“å‡ºæ ¼å¼ï¼‰

3. **æŸ¥çœ‹ç»“æœ**
   - ç»“æœæ˜¾ç¤ºåœ¨é¡µé¢ä¸‹æ–¹
   - æ ¼å¼è½¬æ¢ä¼šæä¾›æ–‡ä»¶ä¸‹è½½é“¾æ¥

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰é…ç½®

```python
config = {
    "summarizer": {
        "use_simple": True,
        "use_small_model": False,
        "model_name": "google/flan-t5-small"
    }
}

processor = DocProcessor(config=config)
```

### å¤„ç†å¤šç§æ–‡æ¡£ç±»å‹

```python
from pathlib import Path

def process_all_documents(directory):
    """å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£"""
    processor = DocProcessor()
    results = {}
    
    for file in Path(directory).rglob("*"):
        if file.suffix in ['.txt', '.md', '.docx', '.pdf']:
            try:
                # ç”Ÿæˆæ‘˜è¦
                summary = processor.generate_summary(file, max_length=200)
                # åˆ†ææ–‡æ¡£
                analysis = processor.analyze(file)
                
                results[str(file)] = {
                    'summary': summary,
                    'word_count': analysis['statistics']['word_count'],
                    'readability': analysis['readability']['score']
                }
            except Exception as e:
                results[str(file)] = {'error': str(e)}
    
    return results
```

### åˆ›å»ºæ–‡æ¡£æŠ¥å‘Š

```python
def create_document_report(document_path, output_path):
    """åˆ›å»ºå®Œæ•´çš„æ–‡æ¡£åˆ†ææŠ¥å‘Š"""
    processor = DocProcessor()
    
    # è·å–åˆ†æç»“æœ
    analysis = processor.analyze(document_path)
    summary = processor.generate_summary(document_path, max_length=300)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = f"""
# æ–‡æ¡£åˆ†ææŠ¥å‘Š

## æ–‡æ¡£ä¿¡æ¯
- æ–‡ä»¶å: {Path(document_path).name}
- å­—æ•°: {analysis['statistics']['word_count']}
- å¥æ•°: {analysis['statistics']['sentence_count']}

## æ‘˜è¦
{summary}

## å¯è¯»æ€§åˆ†æ
- è¯„åˆ†: {analysis['readability']['score']}/100
- å»ºè®®: {analysis['readability']['suggestion']}

## å…³é”®è¯
"""
    for kw in analysis['keywords'][:10]:
        report += f"- {kw['word']} ({kw['frequency']} æ¬¡)\n"
    
    # ä¿å­˜æŠ¥å‘Š
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(report)
```

## æœ€ä½³å®è·µ

### 1. å¤„ç†å¤§æ–‡ä»¶

```python
# å¯¹äºå¤§æ–‡ä»¶ï¼Œå»ºè®®åˆ†æ®µå¤„ç†
def process_large_file(file_path, chunk_size=5000):
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    chunks = [content[i:i+chunk_size] 
              for i in range(0, len(content), chunk_size)]
    
    summaries = []
    for chunk in chunks:
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        with open("temp_chunk.txt", "w", encoding="utf-8") as f:
            f.write(chunk)
        
        summary = processor.generate_summary("temp_chunk.txt", max_length=200)
        summaries.append(summary)
    
    return " ".join(summaries)
```

### 2. é”™è¯¯å¤„ç†

```python
def safe_process(document_path):
    """å¸¦é”™è¯¯å¤„ç†çš„æ–‡æ¡£å¤„ç†"""
    try:
        processor = DocProcessor()
        result = {
            'status': 'success',
            'summary': processor.generate_summary(document_path),
            'analysis': processor.analyze(document_path)
        }
    except FileNotFoundError:
        result = {'status': 'error', 'message': 'æ–‡ä»¶ä¸å­˜åœ¨'}
    except Exception as e:
        result = {'status': 'error', 'message': str(e)}
    
    return result
```

### 3. æ€§èƒ½ä¼˜åŒ–

```python
# å¤ç”¨ processor å®ä¾‹
processor = DocProcessor()

# æ‰¹é‡å¤„ç†æ—¶ä½¿ç”¨æ‰¹é‡æ–¹æ³•
results = processor.batch_process(
    input_dir="docs/",
    output_dir="results/",
    operations=["summarize", "analyze"]
)

# è€Œä¸æ˜¯é€ä¸ªå¤„ç†
# for doc in documents:  # ä¸æ¨è
#     result = processor.generate_summary(doc)
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: ç¿»è¯‘åŠŸèƒ½æŠ¥é”™ï¼Ÿ**
A: ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼ŒGoogle Translate API å¯è®¿é—®ã€‚

**Q: PDF æå–ä¹±ç ï¼Ÿ**
A: æŸäº› PDF æ ¼å¼å¯èƒ½ä¸å®Œå…¨æ”¯æŒï¼Œå»ºè®®å…ˆè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ã€‚

**Q: å†…å­˜ä¸è¶³ï¼Ÿ**
A: å¤„ç†å¤§æ–‡ä»¶æ—¶ï¼Œä½¿ç”¨åˆ†æ®µå¤„ç†æ–¹æ³•ã€‚

**Q: æ‘˜è¦è´¨é‡ä¸ä½³ï¼Ÿ**
A: è°ƒæ•´ max_length å‚æ•°ï¼Œæˆ–ç¡®ä¿åŸæ–‡æ¡£æ ¼å¼æ¸…æ™°ã€‚

## æ›´å¤šèµ„æº

- ğŸ“– [å¿«é€Ÿä¸Šæ‰‹æŒ‡å—](QUICKSTART.md)
- ğŸ’» [ç¤ºä¾‹ä»£ç ](examples/)
- ğŸ› [é—®é¢˜åé¦ˆ](https://github.com/jiangmuran/AIDocGenius/issues)
- ğŸ“§ [è”ç³»æˆ‘ä»¬](mailto:jmr@jiangmuran.com)
